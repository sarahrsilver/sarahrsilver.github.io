[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sarah Silver",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe page your viewing is the “index” page, or the landing page of your site. The site is just a quarto document. And you can put anything on it that could be in a quarto document.\nThere are also settings to get a list of site items you may want people to have easy access to. For example, a list of your blog posts, projects, etc.\nThere are different styles of landing pages. In this particular style, the index.qmd page is used as an about page. Instead of immediately showcasing your work, it showcases you! You can introduce yourself and include your contact information right by your image and description.\nYou can then also decide to have some content show up below the about portion as well."
  },
  {
    "objectID": "posts/Demo Post 1/index.html",
    "href": "posts/Demo Post 1/index.html",
    "title": "Demo Post 1",
    "section": "",
    "text": "This is a demo post in which we begin the blog. The idea here is that you create one post with this quarto document. The quarto document for a post will be named “index.qmd” insides of folder with the name of the post. For example, if I wanted my post to be titled “Demo Post 1” then I would do the following.\nAfter doing that, you can then edit the index.qmd document for that new post to your heart’s content. Lets do a little of that now so you can see how this might work."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#including-resources",
    "href": "posts/Demo Post 1/index.html#including-resources",
    "title": "Demo Post 1",
    "section": "Including Resources",
    "text": "Including Resources\nSuppose you wanted to discuss something, like the CRISP-DM process for analytics projects. You might wish to refer to an image of the process and you could include the image in the “Demo Post 1” folder and reference it here in the document.\n\n\n\n\n\nYou can easily insert the image through the visual editor in Posit / RStudio."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#data-and-output",
    "href": "posts/Demo Post 1/index.html#data-and-output",
    "title": "Demo Post 1",
    "section": "Data and Output",
    "text": "Data and Output\nLets look at some data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(\"USArrests\")\n\nUSArrests %&gt;%\n  ggplot(aes(x = Assault, y = Murder)) +\n  geom_point(pch = 21, color = \"coral3\", bg = \"coral\", size=3) +\n  labs(title = \"Arrests for Murder vs. Assault in US States\",\n       x = \"Arrests for assault per 100,000\",\n       y = \"Arrests for murder per 100,000\") +\n  theme_clean()\n\n\n\n\nThis would show us a relationship that we could then spend some paragraphs analyzing and interpreting."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "National Veterans Organization Donor Classification\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nproblem set 3\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/National Veterans Organization Classification/NVO donor classification.html",
    "href": "posts/National Veterans Organization Classification/NVO donor classification.html",
    "title": "National Veterans Organization Donor Classification",
    "section": "",
    "text": "1. Write a paragraph explaining why classification is the right approach for the NVO’s problem.\nThe NVO is trying to determine whether a person will respond or not respond, which is effectively sorting their donor list into two categories. Classification is a data mining technique which predicts the category of an observation based on predictor variables. By classifying donors into classes respond/not respond, the NVO can tailor their email campaign accordingly, whether this means focusing all of their emails on likely donors, or tailoring specific emails based on how likely the donor is to reply. By using a data-driven approach to their marketing campaign, the NVO can improve their response rate, and ultimately increase the amount of donations they receive.\n\n\n2. Write a paragraph explaining how NVO could use the classifier you build to identify potential donors. Why could it be better than what they’ve been doing?\nBy using the classification model I will build for determining whether a donor will respond to an email or not, based on the factors most correlated with response, including demographics and information about previous donations, the NVO will be able to process donor data effectively and efficiently in order to maximize responses using the least resources possible. Nonprofits often will send out emails or physical mailings to everyone on their lists, wasting their resources on advertisements that will end up in the trash or in the recipient’s spam folder. By leveraging a classification model, the NVO will be able to only send mail/email to those who are likely to respond, allowing them to increase their response rate and save resources in the process.\n\n\n3. Write a paragraph explaining which measures from the confusion matrix you’ll use to evaluate the classifier performance and how they relate to important areas like mailer response rate, and maximizing donation opportunities.\nI plan on using precision, recall/sensitivity, and specificity to evaluate the performance of my classifier.\n\nPrecision will measure the proportion of predicted responders will actually respond. High precision indicates that when the classifier predicts a positive outcome, it is likely to be correct. This leaves a high-precision model open to missing actual positive cases.\nRecall/sensitivity/true positive rate will measure the proportion of actual responders that we predicted would respond. This is important, as we want to contact everyone who will respond, and avoid missing out on potential donations. High sensitivity indicates that the classifier is good at identifying all positive cases, but it may also include false positives.\nSpecificity/true negative rate will measure the proportion of actual non-responders we predicted would not respond. This is important as a high specificity ensures that we are not wasting resources on those who won’t respond. High specificity indicates that the classifier is good at identifying all negative cases, but it may also include false negatives.\nF1-score is a metric defined as the harmonic mean between precision and recall. This helps us make sure our model is balanced in terms of precision and recall – if these two values are significantly different, the F1-score will be low, but if they’re similar, the F1-score will be high. F1-score is especially important in a model where a false positive and false negative have relatively equal impacts, and therefore our best model will balance the two.\n\nAfter perusing and cleaning the data, decide on the most useful features and build the two classification models - remembering to follow proper principles (i.e., data partitioning, cross validation, etc.).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(dummy)\n\ndummy 0.1.3\ndummyNews()\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(performanceEstimation)\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\nAttaching package: 'pROC'\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\n\n\ndonors &lt;- read.csv(\"donors.csv\")\n\n\nhead(donors)\n\n  age numberChildren incomeRating wealthRating mailOrderPurchases\n1  60             NA           NA           NA                  0\n2  46              1            6            9                 16\n3  NA             NA            3            1                  2\n4  70             NA            1            4                  2\n5  78              1            3            2                 60\n6  NA             NA           NA           NA                  0\n  totalGivingAmount numberGifts smallestGiftAmount largestGiftAmount\n1               240          31                  5                12\n2                47           3                 10                25\n3               202          27                  2                16\n4               109          16                  2                11\n5               254          37                  3                15\n6                51           4                 10                16\n  averageGiftAmount yearsSinceFirstDonation monthsSinceLastDonation\n1          7.741935                       8                      14\n2         15.666667                       3                      14\n3          7.481481                       7                      14\n4          6.812500                      10                      14\n5          6.864865                      11                      13\n6         12.750000                       3                      20\n  inHouseDonor plannedGivingDonor sweepstakesDonor P3Donor state urbanicity\n1        FALSE              FALSE            FALSE   FALSE    IL       town\n2        FALSE              FALSE            FALSE   FALSE    CA     suburb\n3        FALSE              FALSE            FALSE   FALSE    NC      rural\n4        FALSE              FALSE            FALSE   FALSE    CA      rural\n5         TRUE              FALSE            FALSE    TRUE    FL     suburb\n6        FALSE              FALSE            FALSE   FALSE    AL       town\n  socioEconomicStatus isHomeowner gender respondedMailing\n1             average          NA female            FALSE\n2             highest        TRUE   male            FALSE\n3             average          NA   male            FALSE\n4             average          NA female            FALSE\n5             average        TRUE female            FALSE\n6             average          NA   &lt;NA&gt;            FALSE\n\nsummary(donors)\n\n      age        numberChildren   incomeRating    wealthRating  \n Min.   : 1.00   Min.   :1.00    Min.   :1.000   Min.   :0.00   \n 1st Qu.:48.00   1st Qu.:1.00    1st Qu.:2.000   1st Qu.:3.00   \n Median :62.00   Median :1.00    Median :4.000   Median :6.00   \n Mean   :61.61   Mean   :1.53    Mean   :3.886   Mean   :5.35   \n 3rd Qu.:75.00   3rd Qu.:2.00    3rd Qu.:5.000   3rd Qu.:8.00   \n Max.   :98.00   Max.   :7.00    Max.   :7.000   Max.   :9.00   \n NA's   :23665   NA's   :83026   NA's   :21286   NA's   :44732  \n mailOrderPurchases totalGivingAmount  numberGifts      smallestGiftAmount\n Min.   :  0.000    Min.   :  13.0    Min.   :  1.000   Min.   :   0.000  \n 1st Qu.:  0.000    1st Qu.:  40.0    1st Qu.:  3.000   1st Qu.:   3.000  \n Median :  0.000    Median :  78.0    Median :  7.000   Median :   5.000  \n Mean   :  3.321    Mean   : 104.5    Mean   :  9.602   Mean   :   7.934  \n 3rd Qu.:  3.000    3rd Qu.: 131.0    3rd Qu.: 13.000   3rd Qu.:  10.000  \n Max.   :241.000    Max.   :9485.0    Max.   :237.000   Max.   :1000.000  \n                                                                          \n largestGiftAmount averageGiftAmount  yearsSinceFirstDonation\n Min.   :   5      Min.   :   1.286   Min.   : 0.000         \n 1st Qu.:  14      1st Qu.:   8.385   1st Qu.: 2.000         \n Median :  17      Median :  11.636   Median : 5.000         \n Mean   :  20      Mean   :  13.348   Mean   : 5.596         \n 3rd Qu.:  23      3rd Qu.:  15.478   3rd Qu.: 9.000         \n Max.   :5000      Max.   :1000.000   Max.   :13.000         \n                                                             \n monthsSinceLastDonation inHouseDonor    plannedGivingDonor sweepstakesDonor\n Min.   : 0.00           Mode :logical   Mode :logical      Mode :logical   \n 1st Qu.:12.00           FALSE:88709     FALSE:95298        FALSE:93795     \n Median :14.00           TRUE :6703      TRUE :114          TRUE :1617      \n Mean   :14.36                                                              \n 3rd Qu.:17.00                                                              \n Max.   :23.00                                                              \n                                                                            \n  P3Donor           state            urbanicity        socioEconomicStatus\n Mode :logical   Length:95412       Length:95412       Length:95412       \n FALSE:93395     Class :character   Class :character   Class :character   \n TRUE :2017      Mode  :character   Mode  :character   Mode  :character   \n                                                                          \n                                                                          \n                                                                          \n                                                                          \n isHomeowner       gender          respondedMailing\n Mode:logical   Length:95412       Mode :logical   \n TRUE:52354     Class :character   FALSE:90569     \n NA's:43058     Mode  :character   TRUE :4843      \n                                                   \n                                                   \n                                                   \n                                                   \n\n\n\n(apply(X = is.na(donors), MARGIN = 2, FUN = sum))\n\n                    age          numberChildren            incomeRating \n                  23665                   83026                   21286 \n           wealthRating      mailOrderPurchases       totalGivingAmount \n                  44732                       0                       0 \n            numberGifts      smallestGiftAmount       largestGiftAmount \n                      0                       0                       0 \n      averageGiftAmount yearsSinceFirstDonation monthsSinceLastDonation \n                      0                       0                       0 \n           inHouseDonor      plannedGivingDonor        sweepstakesDonor \n                      0                       0                       0 \n                P3Donor                   state              urbanicity \n                      0                       0                    2316 \n    socioEconomicStatus             isHomeowner                  gender \n                   2316                   43058                    4676 \n       respondedMailing \n                      0 \n\nsum(!complete.cases(donors))\n\n[1] 88455\n\n\n\nDealing with missing values\nWe have many missing values in our data – 88,455 out of 95412 observations have at least one missing value. For continuous variables, we will perform median imputation. For categorical variables, we will either drop columns or replace NA’s with the most common class.\nnumerical variables with NA’s: age\ncategorical variables with NA’s: numberChildren (mostly NA’s), income rating, wealth rating (about half NA’s), urbanicity, socioEconomicStatus, isHomeowner (about half NA’s), gender\n\nsummary(donors$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00   48.00   62.00   61.61   75.00   98.00   23665 \n\nhistogram(donors$age)\n\n\n\n\nmedian imputation for age variable:\n\ndonors &lt;- donors %&gt;%\n  mutate(age = ifelse(is.na(age),\n                           median(age, na.rm = TRUE),\n                           age))\n\n\nsummary(donors$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   52.00   62.00   61.71   71.00   98.00 \n\nhistogram(donors$age)\n\n\n\n\nEDA for categorical variables with NA’s:\n\n\nisHomeowner\n\ndonors %&gt;%\n  count(isHomeowner, respondedMailing)\n\n  isHomeowner respondedMailing     n\n1        TRUE            FALSE 49646\n2        TRUE             TRUE  2708\n3          NA            FALSE 40923\n4          NA             TRUE  2135\n\n\n5.17% response rate for isHomeowner ==True\n4.96% response rate for isHomeowner == NA\nThe only value for isHomeowner is TRUE. Even if we treat the NA’s as their own category, the response rate is not significantly different for the two groups. We should drop the column.\n\ndonors &lt;- donors %&gt;%\n  select(-isHomeowner)\n\n\n\nnumberChildren\n\ndonors %&gt;%\n  count(numberChildren)\n\n  numberChildren     n\n1              1  7792\n2              2  3110\n3              3  1101\n4              4   316\n5              5    59\n6              6     7\n7              7     1\n8             NA 83026\n\n\nMost of the values for numberChildren are missing. We should drop the column.\n\ndonors &lt;- donors %&gt;%\n  select(-numberChildren)\n\n\n\nincomeRating\n\nincomePred &lt;- donors %&gt;%\n  count(incomeRating, respondedMailing) %&gt;%\n  group_by(incomeRating) %&gt;%\n  mutate(count = sum(n)) %&gt;%\n  group_by(respondedMailing) %&gt;%\n  mutate(respRate = n/count) %&gt;%\n  filter(respondedMailing == TRUE)\nincomePred\n\n# A tibble: 8 × 5\n# Groups:   respondedMailing [1]\n  incomeRating respondedMailing     n count respRate\n         &lt;int&gt; &lt;lgl&gt;            &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n1            1 TRUE               376  9022   0.0417\n2            2 TRUE               632 13114   0.0482\n3            3 TRUE               423  8558   0.0494\n4            4 TRUE               640 12732   0.0503\n5            5 TRUE               812 15451   0.0526\n6            6 TRUE               431  7778   0.0554\n7            7 TRUE               426  7471   0.0570\n8           NA TRUE              1103 21286   0.0518\n\nhistogram(donors$incomeRating)\n\n\n\nplot(x = incomePred$incomeRating, y = incomePred$respRate)\n\n\n\n\nincomeRating is positively correlated with the response rate, so we should keep the variable, even though it has a significant number of NA’s. Conveniently, the response rate of the NA values is right in between the response rate for incomeLevel 4 and 5, and 5 is the most commonly occurring value. We can feel confident that replacing our NA’s with the mode will not have a drastic effect on our model’s performance.\n\ndonors &lt;- donors %&gt;%\n  mutate(incomeRating = ifelse(is.na(incomeRating),\n                           5,\n                           incomeRating))\nhistogram(donors$incomeRating)\n\n\n\n\n\n\nwealthRating\n\nwealthPred &lt;- donors %&gt;%\n  count(wealthRating, respondedMailing) %&gt;%\n  group_by(wealthRating) %&gt;%\n  mutate(count = sum(n)) %&gt;%\n  group_by(respondedMailing) %&gt;%\n  mutate(respRate = n/count) %&gt;%\n  filter(respondedMailing == TRUE)\nwealthPred\n\n# A tibble: 11 × 5\n# Groups:   respondedMailing [1]\n   wealthRating respondedMailing     n count respRate\n          &lt;int&gt; &lt;lgl&gt;            &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1            0 TRUE               111  2413   0.0460\n 2            1 TRUE               162  3454   0.0469\n 3            2 TRUE               202  4085   0.0494\n 4            3 TRUE               216  4237   0.0510\n 5            4 TRUE               237  4810   0.0493\n 6            5 TRUE               254  5280   0.0481\n 7            6 TRUE               323  5825   0.0555\n 8            7 TRUE               330  6198   0.0532\n 9            8 TRUE               350  6793   0.0515\n10            9 TRUE               423  7585   0.0558\n11           NA TRUE              2235 44732   0.0500\n\nhistogram(donors$wealthRating)\n\n\n\nplot(x = wealthPred$wealthRating, y = wealthPred$respRate)\n\n\n\n\nThe choice, whether to impute or drop this column, is trickier than the last example. The relationship between wealthRating and response rate is positively correlated, but with much more irregularity than incomeRating. Additionally, the mode of the wealthRating variable is 9, the highest possible answer. The response rate for the NA’s does not line up with the response rate for the 9’s.\nThis is also a good time to consider multicollinearity. While wealthRating and incomeRating sound like they could be representing the same thing, if we consider our dataset, our donors are likely older and often veterans, who may be retired, with low income yet with high wealth in retirement accounts and other investments. This is why the distribution of wealthRating and incomeRating are so different, and we can be sure that including both of these variables will not introduce multicollinearity into our model. We also have a variable called socioEconomicStatus, which could represent the same information as wealthRating, which could introduce multicollinearity into our model.\nUltimately, I have decided to drop the column. More advanced imputation techniques could be used to fix this problem but this is beyond the scope of this assignment.\n\ndonors &lt;- donors %&gt;%\n  select(-wealthRating)\n\n\n\nurbanicity, socioEconomicStatus, gender\nThese three categorical variables have relatively low proportions of NA values so we don’t need to worry so much about the impact of imputation on our model’s accuracy. I will use mode imputation for all three of these variables.\n\ngetmode &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\ndonors &lt;- donors %&gt;% \n  mutate(urbanicity = if_else(is.na(urbanicity), \n                         getmode(donors$urbanicity), \n                         urbanicity)) %&gt;%\n  mutate(socioEconomicStatus = if_else(is.na(socioEconomicStatus), \n                         getmode(donors$socioEconomicStatus), \n                         socioEconomicStatus)) %&gt;%\n  mutate(gender = if_else(is.na(gender), \n                         getmode(donors$gender), \n                         gender))\n\n\n\nState\nState being a categorical variable with 50 levels may exert undue force on the model, so it’s better to remove it.\n\ndonors &lt;- donors %&gt;%\n  select(-state)\n\n\n\nCorrelation\n\nnum &lt;- donors %&gt;%\n  keep(is.numeric)\ncorr &lt;- cor(num)\ncorrplot(corr)\n\n\n\n\nThe largest correlations are between variables smallestGiftAmount, largestGiftAmount, and averageGiftAmount, as well as between numberGifts and yearsSinceFirstDonation.\nWe will remove smallestGiftAmount, largestGiftAmount, and yearsSinceFirstDonation.\n\n#donors &lt;- donors %&gt;%\n  #select(-c(\"smallestGiftAmount\", \"largestGiftAmount\", \"yearsSinceFirstDonation\"))\n\n\n\nScaling\nLasso models are sensitive to the scale of the input features, so we need to do some scaling.\n\nnormalize &lt;- function(x) {\n  return((x-min(x))/(max(x)-min(x)))\n}\n\ndonors &lt;- donors %&gt;%\n  mutate_at(vars(age, mailOrderPurchases, totalGivingAmount, numberGifts, averageGiftAmount, monthsSinceLastDonation), normalize)\n\n\n\nDummies\n\ndonors$incomeRating &lt;- as.factor(donors$incomeRating)\ndonors_dummies &lt;- dummy(donors)\ndonors_dummies &lt;- donors_dummies %&gt;%\n  mutate_all(as.factor)\ndonors_num &lt;- donors %&gt;% keep(is.numeric)\ndonors_bool &lt;- donors %&gt;% select(c(\"inHouseDonor\", \"plannedGivingDonor\", \"sweepstakesDonor\", \"respondedMailing\"))\ndonors_bool &lt;- donors_bool*1\ndonors_bool &lt;- donors_bool %&gt;%\n  mutate_all(as.factor)\n\ndonors_bool$respondedMailing &lt;- as.factor(donors_bool$respondedMailing)\ndonors_model &lt;- bind_cols(donors_dummies, donors_num, donors_bool)\n\n\n\n\n4. Build a logistic LASSO model using cross-validation on the training data to select the best $\\lambda$. View the coefficients at that chosen $\\lambda$ and see what features are in the model.\n\nset.seed(567)\nsamp = createDataPartition(donors_model$respondedMailing, p = 0.7, list = FALSE)\nlasso_train = donors_model[samp, ]\nlasso_test = donors_model[-samp,]\nrm(samp)\n\n\nset.seed(567)\nlasso_train_down = downSample(x = select(lasso_train, -respondedMailing),\n                         y = lasso_train$respondedMailing,\n                         yname = \"respondedMailing\")\nlasso_train_down %&gt;% select(respondedMailing) %&gt;% table()\n\nrespondedMailing\n   0    1 \n3391 3391 \n\n\n\nset.seed(567)\nlasso_train_smote &lt;- smote(respondedMailing ~ .,\n                   data = lasso_train,\n                   perc.under = 2,\n                   perc.over = 3)\n\nlasso_train_smote %&gt;%\n  select(respondedMailing) %&gt;%\n  table() \n\nrespondedMailing\n    0     1 \n20346 13564 \n\n\n\n#separate predictors and outcome\nx &lt;- model.matrix(respondedMailing~., lasso_train_smote)[,-1]\ny &lt;- lasso_train_smote$respondedMailing\n\n\nset.seed(123) \ncv.lasso &lt;- cv.glmnet(x, y, alpha = 1, family = \"binomial\")\nmodel &lt;- glmnet(x, y, alpha = 1, family = \"binomial\",\n                lambda = cv.lasso$lambda.min)\n#regression coefficients\ncoef(model)\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                                        s0\n(Intercept)                   4.985732e-01\nincomeRating_11              -2.020229e-01\nincomeRating_21              -1.183531e-01\nincomeRating_31              -6.212508e-02\nincomeRating_41               .           \nincomeRating_51               5.771056e-02\nincomeRating_61               1.287037e-01\nincomeRating_71               5.095092e-02\nurbanicity_city1              3.071410e-02\nurbanicity_rural1            -6.652225e-02\nurbanicity_suburb1            7.224272e-02\nurbanicity_town1              .           \nurbanicity_urban1            -3.956056e-02\nsocioEconomicStatus_average1  .           \nsocioEconomicStatus_highest1  9.069761e-02\nsocioEconomicStatus_lowest1  -2.125498e-01\ngender_female1                .           \ngender_joint1                 1.598909e-01\ngender_male1                  .           \nage                          -1.021375e-01\nmailOrderPurchases            2.528882e-01\ntotalGivingAmount            -5.716850e+00\nnumberGifts                   4.957004e+00\nsmallestGiftAmount            .           \nlargestGiftAmount            -4.362408e-04\naverageGiftAmount            -1.867638e+01\nyearsSinceFirstDonation       2.670520e-03\nmonthsSinceLastDonation      -1.241381e+00\ninHouseDonor1                -2.190484e-01\nplannedGivingDonor1           8.129361e-01\nsweepstakesDonor1            -7.918016e-01\n\n\n\n#preds\nx_test &lt;- model.matrix(respondedMailing ~., lasso_test)[,-1]\nprobabilities &lt;- model %&gt;% predict(newx = x_test, type=\"response\")\npredicted_classes &lt;- ifelse(probabilities &gt; .50, \"1\", \"0\")\n#accuracy\nobserved_classes &lt;- lasso_test$respondedMailing\npaste0(\"Model Accuracy: \", mean(predicted_classes == observed_classes))\n\n[1] \"Model Accuracy: 0.873803368038572\"\n\n\nOur final accuracy is 92.6%. All variables with non-zero coefficients are included in the model.\nBy choosing a low threshold of .2, we are allowing our model to predict more positive outcomes. In this case, the cost of a mailing without a response is lower than failing to send a mailing to a donor who would respond.\n\nplot(cv.lasso)\n\n\n\ncv.lasso$lambda.min\n\n[1] 0.0007202813\n\n\nThe optimal \\(\\lambda\\) is .000314.\n\ncoef(cv.lasso, cv.lasso$lambda.min)\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                                        s1\n(Intercept)                   4.993162e-01\nincomeRating_11              -2.021915e-01\nincomeRating_21              -1.185294e-01\nincomeRating_31              -6.224906e-02\nincomeRating_41               .           \nincomeRating_51               5.758178e-02\nincomeRating_61               1.286531e-01\nincomeRating_71               5.089184e-02\nurbanicity_city1              3.126162e-02\nurbanicity_rural1            -6.604364e-02\nurbanicity_suburb1            7.258610e-02\nurbanicity_town1              .           \nurbanicity_urban1            -3.919950e-02\nsocioEconomicStatus_average1  .           \nsocioEconomicStatus_highest1  9.061302e-02\nsocioEconomicStatus_lowest1  -2.125901e-01\ngender_female1                .           \ngender_joint1                 1.599166e-01\ngender_male1                  .           \nage                          -1.020041e-01\nmailOrderPurchases            2.527776e-01\ntotalGivingAmount            -5.614125e+00\nnumberGifts                   4.931489e+00\nsmallestGiftAmount            .           \nlargestGiftAmount            -4.372297e-04\naverageGiftAmount            -1.875743e+01\nyearsSinceFirstDonation       2.652037e-03\nmonthsSinceLastDonation      -1.241342e+00\ninHouseDonor1                -2.193414e-01\nplannedGivingDonor1           8.103401e-01\nsweepstakesDonor1            -7.918323e-01\n\n\n\ntable(predicted_classes,observed_classes)\n\n                 observed_classes\npredicted_classes     0     1\n                0 24774  1216\n                1  2396   236\n\n\n\nplot(roc.glmnet(model, \n                newx = x, \n                newy = y ), \n     type=\"l\")  \n\n\n\n\n\n\n5. Build a decision tree model using cross-validation on the training data to select the best cp value. Use rpart.plot() to view the decision tree. What key features does it use?\n\nClass Balance\n\ndonors_model %&gt;% \n  select(respondedMailing) %&gt;%\n  table()\n\nrespondedMailing\n    0     1 \n90569  4843 \n\n\nOur classes are imbalanced. Only 5.3% of donors responded to the mailing. We need to take this into account in our model.\n\n\nData Partition\n\nset.seed(567)\nsamp = createDataPartition(donors_model$respondedMailing, p = 0.7, list = FALSE)\ntrain = donors_model[samp, ]\ntest = donors_model[-samp,]\nrm(samp)\n\nCheck class balance of test and train sets:\n\ntrain %&gt;% select(respondedMailing) %&gt;% table() %&gt;% prop.table()\n\nrespondedMailing\n         0          1 \n0.94922893 0.05077107 \n\ntest %&gt;% select(respondedMailing) %&gt;% table() %&gt;% prop.table()\n\nrespondedMailing\n         0          1 \n0.94926979 0.05073021 \n\n\nThe degree of imbalance is similar.\n\n\nDownsampling\n\nset.seed(567)\ntrain_down = downSample(x = select(train, -respondedMailing),\n                         y = train$respondedMailing,\n                         yname = \"respondedMailing\")\ntrain_down %&gt;% select(respondedMailing) %&gt;% table()\n\nrespondedMailing\n   0    1 \n3391 3391 \n\n\n\nset.seed(567)\ntrain_smote = smote(respondedMailing ~ .,\n                   data = train,\n                   perc.under = 2,\n                   perc.over = 1.5)\n\ntrain_smote %&gt;%\n  select(respondedMailing) %&gt;%\n  table() \n\nrespondedMailing\n   0    1 \n6782 6782 \n\n\n\n\nDecision Tree\n\nctrl = caret::trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\n\nset.seed(567)\nunbalanced_tree = train(respondedMailing ~ .,\n                        data = train,\n                        method = \"rpart\",\n                        metric = \"Kappa\",\n                        trControl = ctrl,\n                        tuneGrid = expand.grid(cp = seq(0.0, 0.03, 0.0005)))\n\nplot(unbalanced_tree)\n\n\n\n\n\nset.seed(567)\ndown_tree = train(respondedMailing ~ .,\n                        data = train_down,\n                        method = \"rpart\",\n                        metric = \"Kappa\",\n                        #control = rpart_ctrl,\n                        trControl = ctrl,\n                        tuneGrid = expand.grid(cp = seq(0.0, 0.03, 0.0005)))\n\nplot(down_tree)\n\n\n\n\n\nset.seed(567)\nsmote_tree = train(respondedMailing ~ .,\n                        data = train_smote,\n                        method = \"rpart\",\n                        metric = \"Kappa\",\n                        #control = rpart_ctrl,\n                        trControl = ctrl,\n                        tuneGrid = expand.grid(cp = seq(0.0, 0.03, 0.0005)))\n\nplot(smote_tree)\n\n\n\n\n\nrpart.plot(unbalanced_tree$finalModel)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\nrpart.plot(down_tree$finalModel)\n\n\n\n\n\nrpart.plot(smote_tree$finalModel)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\n\n\n6. Evaluate the performance on test data and look at and describe its performance according to your confusion matrix measures.\n\nDecision Trees\n\n# Get class predictions\nunbalanced_test_class = predict(unbalanced_tree, newdata = test, type = \"raw\")\ndown_test_class = predict(down_tree, newdata = test, type = \"raw\")\nsmote_test_class = predict(smote_tree, newdata = test, type = \"raw\")\n\n# Get probability predictions\nunbalanced_test_prob = predict(unbalanced_tree, newdata = test, type = \"prob\")[,2]\ndown_test_prob = predict(down_tree, newdata = test, type = \"prob\")[,2]\nsmote_test_prob = predict(smote_tree, newdata = test, type = \"prob\")[,2]\n\n\npred_prob = predict(smote_tree, newdata = test, type = \"prob\")[,2]\npred_class = factor(ifelse(pred_prob &gt; 0.5, \"1\", \"0\"))\nconfusionMatrix(pred_class, test$respondedMailing, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 21718  1057\n         1  5452   395\n                                          \n               Accuracy : 0.7726          \n                 95% CI : (0.7677, 0.7774)\n    No Information Rate : 0.9493          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0293          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.27204         \n            Specificity : 0.79934         \n         Pos Pred Value : 0.06756         \n         Neg Pred Value : 0.95359         \n             Prevalence : 0.05073         \n         Detection Rate : 0.01380         \n   Detection Prevalence : 0.20428         \n      Balanced Accuracy : 0.53569         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\nunbalanced_cv_kappa = mean(unbalanced_tree$results$Kappa)\nunbalanced_test_kappa = confusionMatrix(unbalanced_test_class,\n                                        test$respondedMailing,\n                                        positive = \"1\")$overall[[\"Kappa\"]]\nunbalanced_test_auc = ModelMetrics::auc(test$respondedMailing, unbalanced_test_prob)\n\ndown_cv_kappa = mean(down_tree$results$Kappa)\ndown_test_kappa = confusionMatrix(down_test_class,\n                                  test$respondedMailing,\n                                  positive = \"1\")$overall[[\"Kappa\"]]\ndown_test_auc = ModelMetrics::auc(test$respondedMailing, down_test_prob)\n\nsmote_cv_kappa = mean(smote_tree$results$Kappa)\nsmote_test_kappa = confusionMatrix(smote_test_class,\n                                   test$respondedMailing,\n                                   positive = \"1\",)$overall[[\"Kappa\"]]\nsmote_test_auc = ModelMetrics::auc(test$respondedMailing, smote_test_prob)\n\n\ntibble(\"CV Kappa\" = c(unbalanced_cv_kappa, down_cv_kappa, smote_cv_kappa),\n       \"Test Kappa\" = c(unbalanced_test_kappa, down_test_kappa, smote_test_kappa),\n       \"Test AUC\" = c(unbalanced_test_auc, down_test_auc, smote_test_auc),\n       \"Tree\" = c(\"Unbalanced\", \"Down\", \"SMOTE\")) %&gt;%\n  column_to_rownames(var = \"Tree\")\n\n               CV Kappa  Test Kappa  Test AUC\nUnbalanced 0.0001703496 0.004668208 0.5728789\nDown       0.0941862279 0.024359487 0.5761872\nSMOTE      0.2145962242 0.029342077 0.5463236\n\n\n\n\n\n7. Create a ROC plot (with AUC) to compare the two model’s performance and explain to NVO what the plot tells you.\n\npar(pty=\"s\")\nunbalanced_roc = roc(test$respondedMailing ~ unbalanced_test_prob, \n                     plot=TRUE, print.auc=TRUE, \n                     col=\"green\", lwd=3, legacy.axes=TRUE)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\ndown_roc = roc(test$respondedMailing ~ down_test_prob,\n               plot=TRUE, print.auc=TRUE, print.auc.y=0.4,\n               col = \"blue\", lwd=3, legacy.axes=TRUE, add=TRUE)\n\nSetting levels: control = 0, case = 1\nSetting direction: controls &lt; cases\n\nsmote_roc = roc(test$respondedMailing ~ smote_test_prob,\n                plot=TRUE, print.auc=TRUE, print.auc.y=0.3,\n                col = \"black\", lwd=3, legacy.axes=TRUE, add=TRUE)\n\nSetting levels: control = 0, case = 1\nSetting direction: controls &lt; cases\n\nlegend(\"bottomright\", legend=c(\"Unbalanced Data\", \"Downsampled Data\", \"SMOTE Data\"),\n       col = c(\"green\", \"blue\", \"black\"), cex = .75, lwd=3)\n\n\n\n\n\n\n8. Pick the best performing model, and view its precision recall chart and its cumulative gain chart.\n\nunbalanced_pred &lt;- predict(unbalanced_tree, newdata = test, type=\"prob\")\ndown_pred &lt;- predict(down_tree, newdata = test, type=\"prob\")\nsmote_pred &lt;- predict(smote_tree, newdata = test, type=\"prob\")\n\nunbalanced_pred$pred &lt;- ifelse(unbalanced_pred[1] &gt; unbalanced_pred[2], 0, 1)\nunbalanced_pred$pred &lt;- as.factor(unbalanced_pred$pred)\n\ndown_pred$pred &lt;- ifelse(down_pred[1] &gt; down_pred[2], 0, 1)\ndown_pred$pred &lt;- as.factor(down_pred$pred)\n\nsmote_pred$pred &lt;- ifelse(smote_pred[1] &gt; smote_pred[2], 0, 1)\nsmote_pred$pred &lt;- as.factor(smote_pred$pred)\n\n\nconfusionMatrix(unbalanced_pred$pred, test$respondedMailing, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 27016  1440\n         1   154    12\n                                          \n               Accuracy : 0.9443          \n                 95% CI : (0.9416, 0.9469)\n    No Information Rate : 0.9493          \n    P-Value [Acc &gt; NIR] : 0.9999          \n                                          \n                  Kappa : 0.0045          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.0082645       \n            Specificity : 0.9943320       \n         Pos Pred Value : 0.0722892       \n         Neg Pred Value : 0.9493956       \n             Prevalence : 0.0507302       \n         Detection Rate : 0.0004193       \n   Detection Prevalence : 0.0057997       \n      Balanced Accuracy : 0.5012982       \n                                          \n       'Positive' Class : 1               \n                                          \n\nconfusionMatrix(down_pred$pred, test$respondedMailing, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 15544   669\n         1 11626   783\n                                          \n               Accuracy : 0.5704          \n                 95% CI : (0.5647, 0.5762)\n    No Information Rate : 0.9493          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0244          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.53926         \n            Specificity : 0.57210         \n         Pos Pred Value : 0.06310         \n         Neg Pred Value : 0.95874         \n             Prevalence : 0.05073         \n         Detection Rate : 0.02736         \n   Detection Prevalence : 0.43355         \n      Balanced Accuracy : 0.55568         \n                                          \n       'Positive' Class : 1               \n                                          \n\nconfusionMatrix(smote_pred$pred, test$respondedMailing, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 21718  1057\n         1  5452   395\n                                          \n               Accuracy : 0.7726          \n                 95% CI : (0.7677, 0.7774)\n    No Information Rate : 0.9493          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0293          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.27204         \n            Specificity : 0.79934         \n         Pos Pred Value : 0.06756         \n         Neg Pred Value : 0.95359         \n             Prevalence : 0.05073         \n         Detection Rate : 0.01380         \n   Detection Prevalence : 0.20428         \n      Balanced Accuracy : 0.53569         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n\n9. Use the charts from parts 6 and 7 to describe how the model should perform for NVO and what it could mean if they do a mailer campaign for 50,000 people."
  },
  {
    "objectID": "docs/posts/Toyota Corolla Pricing Decision Tree/problem set 3.html",
    "href": "docs/posts/Toyota Corolla Pricing Decision Tree/problem set 3.html",
    "title": "problem set 3",
    "section": "",
    "text": "#check if cols have more than one unique value\ncars_check &lt;- cars %&gt;%\nselect_if(function(col) length(unique(col))==1)\ncolnames(cars_check)\n\n[1] \"Cylinders\"\n\n#remove unnecessary cols\ncars &lt;- cars %&gt;%\n  select(-c(Id, Model, Mfg_Month, Mfg_Year, Cylinders))\n\n#change cat variables to factor dt\ncars_fct = cars %&gt;%\n  select(-Price, -Age_08_04, -KM, -HP, -CC, -Quarterly_Tax, -Weight) %&gt;%\n  mutate_all(.funs = factor)\n\ncars_num = cars %&gt;%\n  select(Price, Age_08_04, KM, HP, CC, Quarterly_Tax, Weight)\n\ncars2 &lt;- bind_cols(cars_num, cars_fct)\n\nWe remove columns id and model because these are identifying factors that have no predictive power (model may have some predictive power, but since they’re all Corollas and other information is stored in other columns, we drop it). We remove Mfg_Month and Mfg_Year because we have other columns that tell us how old the car is in Age_08_04. We remove Cylinders because all Corollas have 4 cylinders. We also change all categorical variables to factor datatype.\n\nExplore the data and determine the number of variables and the quantity of any missing values. If values are missing, prescribe a plan to deal with the problem.\n\nglimpse(cars2)\n\nRows: 1,436\nColumns: 34\n$ Price             &lt;int&gt; 13500, 13750, 13950, 14950, 13750, 12950, 16900, 186…\n$ Age_08_04         &lt;int&gt; 23, 23, 24, 26, 30, 32, 27, 30, 27, 23, 25, 22, 25, …\n$ KM                &lt;int&gt; 46986, 72937, 41711, 48000, 38500, 61000, 94612, 758…\n$ HP                &lt;int&gt; 90, 90, 90, 90, 90, 90, 90, 90, 192, 69, 192, 192, 1…\n$ CC                &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 1800…\n$ Quarterly_Tax     &lt;int&gt; 210, 210, 210, 210, 210, 210, 210, 210, 100, 185, 10…\n$ Weight            &lt;int&gt; 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245, 1185…\n$ Fuel_Type         &lt;fct&gt; Diesel, Diesel, Diesel, Diesel, Diesel, Diesel, Dies…\n$ Met_Color         &lt;fct&gt; 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1…\n$ Color             &lt;fct&gt; Blue, Silver, Blue, Black, Black, White, Grey, Grey,…\n$ Automatic         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Doors             &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ Gears             &lt;fct&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5…\n$ Mfr_Guarantee     &lt;fct&gt; 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0…\n$ BOVAG_Guarantee   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Guarantee_Period  &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 12, 3, 3, 3, 3, 3, 3, …\n$ ABS               &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_1          &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Airbag_2          &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Airco             &lt;fct&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Automatic_airco   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Boardcomputer     &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ CD_Player         &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0…\n$ Central_Lock      &lt;fct&gt; 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Powered_Windows   &lt;fct&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Power_Steering    &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Radio             &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Mistlamps         &lt;fct&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Sport_Model       &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0…\n$ Backseat_Divider  &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ Metallic_Rim      &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0…\n$ Radio_cassette    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ Parking_Assistant &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Tow_Bar           &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n\ncars2 %&gt;% summarise(across(everything(), ~ sum(is.na(.))))\n\n  Price Age_08_04 KM HP CC Quarterly_Tax Weight Fuel_Type Met_Color Color\n1     0         0  0  0  0             0      0         0         0     0\n  Automatic Doors Gears Mfr_Guarantee BOVAG_Guarantee Guarantee_Period ABS\n1         0     0     0             0               0                0   0\n  Airbag_1 Airbag_2 Airco Automatic_airco Boardcomputer CD_Player Central_Lock\n1        0        0     0               0             0         0            0\n  Powered_Windows Power_Steering Radio Mistlamps Sport_Model Backseat_Divider\n1               0              0     0         0           0                0\n  Metallic_Rim Radio_cassette Parking_Assistant Tow_Bar\n1            0              0                 0       0\n\n\nWe have 35 columns with 1436 rows. There are no missing values so imputation is not necessary.\n\n\nAnalyze whether the Price variable is appropriate for a linear regression model and discuss its distribution. Are there any transformations that we might apply to the price variable?\n\nden &lt;- density(cars2$Price)\n \nplot(den, frame = FALSE, col = \"red\",main = \"Density of Price\")\n\n\n\n\nThe Price variable is heavily skewed right, so we want to do a log transformation.\n\ncars2$logprice &lt;- log(cars2$Price)\n\ndenlog &lt;- density(cars2$logprice)\n\nplot(denlog, col = \"blue\", main = \"Density of Log Price\")\n\n\n\n\n\n\nIs there a relationship between any of the features in the data and the Price feature? Perform some exploratory analysis to determine some features that are related using a feature plot.\n\nfeaturePlot(x = cars2[, c(\"KM\", \"HP\", \"Age_08_04\", \"Weight\", \"CC\", \"Quarterly_Tax\")], \n            y=cars2$Price, \n            plot  = \"scatter\", \n            type = c(\"p\", \"smooth\"))\n\n\n\n\n\ncorrplot(cor(cars_num))\n\n\n\n\nThe variable with the strongest linear relationship to Price is Age_08_04. This makes sense, as it’s one of the major factors people consider when buying a car. KM also has a strong negative linear relationship with Price for the same reason. These factors also have a somewhat significant positive correlation with each other – the older a car is, the more mileage has likely been put on it.\n\n\nAre there any predictor variables in the data that are potentially too strongly related to each other? Make sure to use reference any visualizations, tables, or numbers to show this.\n\nx &lt;-model.matrix(~., data=cars2) %&gt;% \n  cor(use=\"pairwise.complete.obs\")\n\nx[x == 1] &lt;- NA #drop perfect\nx[abs(x) &lt; 0.7] &lt;- NA # drop less than abs(0.7)\nx &lt;- na.omit(melt(x)) # melt\nx &lt;- x[order(-abs(x$value)),] # sort\nkable(x) %&gt;%\n  kable_styling()\n\n\n\n\n\nX1\nX2\nvalue\n\n\n\n\n2685\nRadio_cassette1\nRadio1\n0.9916210\n\n\n2960\nRadio1\nRadio_cassette1\n0.9916210\n\n\n112\nlogprice\nPrice\n0.9774223\n\n\n3082\nPrice\nlogprice\n0.9774223\n\n\n1427\nGears6\nGears5\n-0.9657999\n\n\n1482\nGears5\nGears6\n-0.9657999\n\n\n458\nFuel_TypePetrol\nFuel_TypeDiesel\n-0.9429759\n\n\n513\nFuel_TypeDiesel\nFuel_TypePetrol\n-0.9429759\n\n\n168\nlogprice\nAge_08_04\n-0.8767022\n\n\n3083\nAge_08_04\nlogprice\n-0.8767022\n\n\n59\nAge_08_04\nPrice\n-0.8765905\n\n\n114\nPrice\nAge_08_04\n-0.8765905\n\n\n2510\nPowered_Windows1\nCentral_Lock1\n0.8755525\n\n\n2565\nCentral_Lock1\nPowered_Windows1\n0.8755525\n\n\n346\nFuel_TypePetrol\nQuarterly_Tax\n-0.8354517\n\n\n511\nQuarterly_Tax\nFuel_TypePetrol\n-0.8354517\n\n\n1200\nDoors5\nDoors3\n-0.8221205\n\n\n1310\nDoors3\nDoors5\n-0.8221205\n\n\n345\nFuel_TypeDiesel\nQuarterly_Tax\n0.7927262\n\n\n455\nQuarterly_Tax\nFuel_TypeDiesel\n0.7927262\n\n\n155\nBoardcomputer1\nAge_08_04\n-0.7194487\n\n\n2355\nAge_08_04\nBoardcomputer1\n-0.7194487\n\n\n\n\n\n\n\nTo check for correlations across both numeric and factor variables, I used the model.matrix function with cor to create a correlation matrix that automatically one-hot encodes factor variables. I then created a dataframe that shows the correlations over my chosen significance threshold of 0.7. It returned 22 rows, but each correlation is doubled (a:b and b:a) so there are 11 highly correlated variables. Several of these are dummies of the same factor, including Gears5:Gears6 and Fuel_TypePetrol:Fuel_TypeDiesel. For a simple linear regression, you would want to address these issues via principal component analysis, but you could also use a model that incorporates feature selection like Lasso regression or a regression tree to fix this problem. Radio_cassette1:Radio1 is likely a measure of the same thing (i.e. all Corolla radios might have a cassette player). For this exercise I will remove Radio_cassette1.\n\n\nPartition your data into a training set with 70% of the observations and a testing set with the remaining 30%.\n\ncar_dum = dummy(cars2, int = TRUE)\ncar_num = cars2 %&gt;%\n  keep(is.numeric)\ncars_model &lt;- bind_cols(car_num, car_dum)\nrm(car_dum, car_num)\n\nzeros &lt;- colnames(cars_model)[endsWith(colnames(cars_model), \"0\")]\n\ncars_model &lt;- cars_model %&gt;%\n  select(-c(logprice, Radio_cassette_1)) %&gt;%\n  select(-all_of(zeros))\n\nset.seed(1111)\nsamp = createDataPartition(cars_model$Price, p = 0.70, list = FALSE)\ntraining = cars_model[samp, ]\ntesting = cars_model[-samp, ]\nrm(samp)\n\nSince we’re making a regression tree, the preparations we made for linear regression were not necessary. I did not remove any correlated variables and I removed the logprice variable and will just be using the original Price variable as the response. I only removed the dummies of the binary variables – others shouldn’t matter as the model performs its own feature selection.\n\n\nBased on your results and relationships in questions (4) and (5), build a regression tree model to predict car prices. Make sure to conduct cross validation to evaluate the model and choose the best cost complexity parameter for this problem (use default values for minsplit, minbucket, maxdepth, etc. But choose grid of cp values to tune over). Use rpart.plot to view your tree and discuss its complexity, usefulness, etc. What role is pre-pruning and post-pruning playing here?\n\ntrain_ctrl = trainControl(method = \"repeatedcv\", number = 20, repeats = 10)\ntree = train(Price~.,\n             data = training,\n             method = \"rpart\",\n             trControl = train_ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.01)),\n             control = rpart.control(method = \"anova\", minsplit = 1, minbucket = 1)\n             )\ntree\n\nCART \n\n1007 samples\n  56 predictor\n\nNo pre-processing\nResampling: Cross-Validated (20 fold, repeated 10 times) \nSummary of sample sizes: 957, 956, 956, 957, 957, 956, ... \nResampling results across tuning parameters:\n\n  cp    RMSE      Rsquared   MAE     \n  0.00  1435.036  0.8389156  1087.655\n  0.01  1407.227  0.8358503  1053.808\n  0.02  1540.777  0.8038972  1146.253\n  0.03  1651.892  0.7747854  1257.275\n  0.04  1651.892  0.7747854  1257.275\n  0.05  1651.892  0.7747854  1257.275\n  0.06  1651.892  0.7747854  1257.275\n  0.07  1651.892  0.7747854  1257.275\n  0.08  1651.892  0.7747854  1257.275\n  0.09  1651.892  0.7747854  1257.275\n  0.10  1651.892  0.7747854  1257.275\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.01.\n\n\n\nplot(tree)\n\n\n\n\nRMSE is minimized when hyperparameter cp = 0.01. Choosing hyperparameters is a part of pre-pruning.\n\nrparttree &lt;- rpart(Price~., data = training, method = \"anova\", control = (cp = 0.01))\nrpart.plot(rparttree)\n\n\n\n\n\n\nLook at the feature importance (using permuted feature importance in “iml” package, with loss = “rmse” and compare = “ratio”) and determine which features have the biggest effect, and which might be okay to remove.\n\ntree_predictor = iml::Predictor$new(tree, data = training)\ntree_imp = iml::FeatureImp$new(tree_predictor, loss = \"rmse\", compare = \"ratio\", n.repetitions = 30)\nplot(tree_imp)\n\n\n\n\n\ntree_imp$results\n\n               feature importance.05 importance importance.95 permutation.error\n1            Age_08_04      3.233959   3.312704      3.435833          4461.975\n2               Weight      1.138150   1.160294      1.181436          1562.832\n3                   KM      1.000000   1.000000      1.000000          1346.928\n4                   HP      1.000000   1.000000      1.000000          1346.928\n5                   CC      1.000000   1.000000      1.000000          1346.928\n6        Quarterly_Tax      1.000000   1.000000      1.000000          1346.928\n7        Fuel_Type_CNG      1.000000   1.000000      1.000000          1346.928\n8     Fuel_Type_Diesel      1.000000   1.000000      1.000000          1346.928\n9     Fuel_Type_Petrol      1.000000   1.000000      1.000000          1346.928\n10         Met_Color_1      1.000000   1.000000      1.000000          1346.928\n11         Color_Beige      1.000000   1.000000      1.000000          1346.928\n12         Color_Black      1.000000   1.000000      1.000000          1346.928\n13          Color_Blue      1.000000   1.000000      1.000000          1346.928\n14         Color_Green      1.000000   1.000000      1.000000          1346.928\n15          Color_Grey      1.000000   1.000000      1.000000          1346.928\n16           Color_Red      1.000000   1.000000      1.000000          1346.928\n17        Color_Silver      1.000000   1.000000      1.000000          1346.928\n18        Color_Violet      1.000000   1.000000      1.000000          1346.928\n19         Color_White      1.000000   1.000000      1.000000          1346.928\n20        Color_Yellow      1.000000   1.000000      1.000000          1346.928\n21         Automatic_1      1.000000   1.000000      1.000000          1346.928\n22             Doors_2      1.000000   1.000000      1.000000          1346.928\n23             Doors_3      1.000000   1.000000      1.000000          1346.928\n24             Doors_4      1.000000   1.000000      1.000000          1346.928\n25             Doors_5      1.000000   1.000000      1.000000          1346.928\n26             Gears_3      1.000000   1.000000      1.000000          1346.928\n27             Gears_4      1.000000   1.000000      1.000000          1346.928\n28             Gears_5      1.000000   1.000000      1.000000          1346.928\n29             Gears_6      1.000000   1.000000      1.000000          1346.928\n30     Mfr_Guarantee_1      1.000000   1.000000      1.000000          1346.928\n31   BOVAG_Guarantee_1      1.000000   1.000000      1.000000          1346.928\n32  Guarantee_Period_3      1.000000   1.000000      1.000000          1346.928\n33  Guarantee_Period_6      1.000000   1.000000      1.000000          1346.928\n34 Guarantee_Period_12      1.000000   1.000000      1.000000          1346.928\n35 Guarantee_Period_13      1.000000   1.000000      1.000000          1346.928\n36 Guarantee_Period_18      1.000000   1.000000      1.000000          1346.928\n37 Guarantee_Period_24      1.000000   1.000000      1.000000          1346.928\n38 Guarantee_Period_28      1.000000   1.000000      1.000000          1346.928\n39 Guarantee_Period_36      1.000000   1.000000      1.000000          1346.928\n40               ABS_1      1.000000   1.000000      1.000000          1346.928\n41          Airbag_1_1      1.000000   1.000000      1.000000          1346.928\n42          Airbag_2_1      1.000000   1.000000      1.000000          1346.928\n43             Airco_1      1.000000   1.000000      1.000000          1346.928\n44   Automatic_airco_1      1.000000   1.000000      1.000000          1346.928\n45     Boardcomputer_1      1.000000   1.000000      1.000000          1346.928\n46         CD_Player_1      1.000000   1.000000      1.000000          1346.928\n47      Central_Lock_1      1.000000   1.000000      1.000000          1346.928\n48   Powered_Windows_1      1.000000   1.000000      1.000000          1346.928\n49    Power_Steering_1      1.000000   1.000000      1.000000          1346.928\n50             Radio_1      1.000000   1.000000      1.000000          1346.928\n51         Mistlamps_1      1.000000   1.000000      1.000000          1346.928\n52       Sport_Model_1      1.000000   1.000000      1.000000          1346.928\n53  Backseat_Divider_1      1.000000   1.000000      1.000000          1346.928\n54      Metallic_Rim_1      1.000000   1.000000      1.000000          1346.928\n55 Parking_Assistant_1      1.000000   1.000000      1.000000          1346.928\n56           Tow_Bar_1      1.000000   1.000000      1.000000          1346.928\n\n\nThe only important variables are Age_08_04 and Weight. Theoretically, we could remove all of the other variables.\n\n\nParsimony is about obtaining the simplest model possible, without oversimplifying. Remove a few of the less useful features and retrain / cross validate / tune your tree.\n\nprunetree &lt;- prune.rpart(tree, cp = .01)\nprunetree\n\nCART \n\n1007 samples\n  56 predictor\n\nNo pre-processing\nResampling: Cross-Validated (20 fold, repeated 10 times) \nSummary of sample sizes: 957, 956, 956, 957, 957, 956, ... \nResampling results across tuning parameters:\n\n  cp    RMSE      Rsquared   MAE     \n  0.00  1435.036  0.8389156  1087.655\n  0.01  1407.227  0.8358503  1053.808\n  0.02  1540.777  0.8038972  1146.253\n  0.03  1651.892  0.7747854  1257.275\n  0.04  1651.892  0.7747854  1257.275\n  0.05  1651.892  0.7747854  1257.275\n  0.06  1651.892  0.7747854  1257.275\n  0.07  1651.892  0.7747854  1257.275\n  0.08  1651.892  0.7747854  1257.275\n  0.09  1651.892  0.7747854  1257.275\n  0.10  1651.892  0.7747854  1257.275\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.01.\n\n\nThe pruning function did not remove any features. We already have the simplest model possible.\n\n\nUse the model resulting from question 9 and test predictions on the testing data. Compare the cross validation error and and testing data. Spend some time interpreting what this prediction error means for your pricing model and its use for CorollaCrowd.\n\ntraining$pred &lt;- predict(prunetree, training)\ntraining$resid &lt;- (training$pred - training$Price)\naccuracy(training$pred, training$Price)\n\n                    ME     RMSE      MAE       MPE     MAPE\nTest set -1.692786e-13 1346.928 1009.732 -1.819971 10.24208\n\ntesting$pred &lt;- predict(prunetree, testing)\ntesting$resid &lt;- (testing$pred - testing$Price)\naccuracy(testing$pred, testing$Price)\n\n               ME    RMSE      MAE       MPE     MAPE\nTest set 83.21296 1562.93 1057.476 -1.250146 10.05107\n\n\nThe RMSE on the test set is higher than the RMSE on the training set, but not by much.\n\nplot(training$pred, training$resid)\n\n\n\nplot(testing$pred, testing$resid)\n\n\n\n\nResiduals are centered fairly well around 0 for the training data. In the testing data, there are some outliers in the most expensive Corollas, as the model never predicts a value of over $22714.17.\n\nmax(training$Price)\n\n[1] 24990\n\nmax(testing$Price)\n\n[1] 32500\n\n\nThe maximum Price in the training set is $24990 while the maximum Price in the testing set is $32500, so it makes sense why the model failed to capture these higher value cars. The model is most accurate for lower priced Corollas, as the distribution is skewed right."
  }
]